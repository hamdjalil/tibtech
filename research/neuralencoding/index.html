<!DOCTYPE html>
<html lang="en">
<head>
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-38PESE35X3"></script>
    <script>
      window.dataLayer = window.dataLayer || [];
      function gtag(){dataLayer.push(arguments);}
      gtag('js', new Date());
      gtag('config', 'G-38PESE35X3');
    </script>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Neural Encoding</title>
    <link rel="stylesheet" href="style.css">
    <link href="https://fonts.googleapis.com/css2?family=Roboto:wght@300;400;700&family=Lora:wght@400;700&display=swap" rel="stylesheet">
    <link href="https://fonts.googleapis.com/css2?family=Inter+Tight:wght@300;400;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.5.2/css/all.min.css">
    

</head>
<body style="background-color: rgb(255, 255, 255); padding-top: 100px;">
    <a href="https://tibbtech.com/" target="_blank">
    <div id="navbar" class="noto">
            <img src="logo.svg" style="height: 100%;" />
            <!-- <div onclick="toggleNav(event)">
                <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" style="width: 30px; height: 30px; cursor: pointer">
                    <path stroke-linecap="round" stroke-linejoin="round" d="M3.75 6.75h16.5M3.75 12h16.5m-16.5 5.25h16.5" />
                </svg>              
            </div> -->
        </div>
    </a>
    <!-- <div id="hidden-nav"> 
        <div onclick="toggleNav()" style="position: absolute; top: 10px; right: 30px">
            <svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 24 24" stroke-width="1.5" stroke="currentColor" style="width: 20px; height: 20px;">
                <path stroke-linecap="round" stroke-linejoin="round" d="M6 18 18 6M6 6l12 12" />
            </svg>              
        </div>  
        <div onclick="scrollEle('about')">About</div>
        <div onclick="scrollEle('contact-section')">Contact</div>
        <div onclick="scrollEle('team')">Team</div>
    </div> -->

    <main class="main-container">
        <section class="hero">
            <h1>Natural Language-guided Neural Encoding Benchmark<br>for Vision</h1>
            <!-- <p class="title"> Multimodal Vision Framework for Behavior-based Precision Livestock Farming </p> -->
            <!-- <p class="author-info">Dec 08, 2024</p> -->
            <p class="author-info">By Taha, Hisan & Asim <br> Tibbling Technologies</p>
            <!-- <div class="slider">
                <img src="AnimalFormer_figures/AnimalFormer_Updated_Sheep_Only_Faster.gif" alt="Main Image" class="main-image"> -->
                <!-- <video controls autoplay muted loop id="AnimalFormer_video" class = "video" >
                    <source src="AnimalFormer_figures/AnimalFormer Animation.mp4" type="video/mp4">
                    Your browser does not support the video tag.
                </video> -->
            <!-- </div> --> 
        </section>
        <section class="content">
            <article class="post">
                <div class="post-container">
                    <div class="post-text-2">
                        <!-- <h3>What's under the hood?</h3> -->
                        <p>
                            A longstanding question in neuroscience has been the quest to uncover patterns of neuronal activity in response to specific visual stimuli. Neurons function as individual sensory units with distinct roles, responding to their preferred stimuli, which is determined by their receptive fields. This relationship leads to recognizable patterns of neuronal activity in the brain when exposed to external stimuli. Visual neurons, in particular, tend to encode unique visual stimuli, especially for object classification, suggesting a causal relationship between visual inputs and neural activity in the visual cortex. By identifying the most active neurons corresponding to specific visual objects, we can develop a modeling framework to decode neuronal activity related to the encoding of imagined visual images.
                        </p>
                        <br>
                        <p>
                            An idea that leverages this mapping between visual stimuli and neurons is language-guided vision, a technique aiming to aid visually-impaired subjects by describing visual stimuli in a natural language, relayed to them via a text-driven speech system. This can potentially enable us to simulate the function of the human visual system using language-guided input, providing subjects with an improved visual experience by activating the same neurons in their visual cortex.
c:\Users\tahar\OneDrive - Higher Education Commission\Documents\GitHub\tibtech\index.html
                        </p>
                    </div>
                   
                    
                </div>
            </article>

            <article class="post">
                <div class="post-container">
                    <div class="post-text-2">
                        <h3>Introducing Neural Encoding Benchmark </h3>
                        <p>
                            We are excited to introduce a novel benchmarking mapping technique that allows us to quantitatively measure the performance of image2text models, using a unique mapping mechanism between the neurons from the visual cortex and text tokens generated by the image captioning models. 
                        </p>
                    </div>
                    <br>
                    <img src="figures/Figure_1_NeuroGPT_CVPR.drawio.svg" alt="Image 1" class="inline-image">
                    <p class="caption">
                        Figure 1: Our integrated framework, designed for benchmarking state-of-the-art image2text models demonstrated on a mouse and primate datasets. This block diagram provides a clear overview of the data flow and processing steps within our novel neural encoding benchmarking technique.
                    </p>
                    <div class="post-text-2">
                        <p>
                            Our benchmarking framework is one of the pioneering approaches to systematically compare the neural encoding in visual cortex with the visual representations generated by the image captioning models to facilitate towards the language-guided visual aid applications for visually impaired subjects with a range from poor vision to blindness. 
                        </p>
                        <br>
                        <p>
                            We shows that the neural responses from the visual cortex are systematically mapped to text tokens generated by image2text models. Our evaluation criteria is unique, and compares the image2text models with the functioning of the visual cortex.
                            For applications like natural language-guided visual therapy (empowered by speech), this proves to be of immense importance, as it provides both qualitative and quantitative evaluation metrics for benchmarking the image2text models on the mammalian visual cortex. 
                        </p>
                        
                    </div>
                    
                </div>
            </article>

            <article class="post">
                <div class="post-container">
                    <div class="post-text-2">
                        <h2>Cross-species Benchmark</h2>
                        <p> We demonstrate the effectiveness of our approach by utilizing functional data from the visual cortex of both primates and mice, ensuring that our technique yields consistent results across species. The mouse visual cortex serves as an excellent model for this investigation due to its well-studied nature and shared properties with the primate visual cortex, making it amenable to experimental manipulation. This allows researchers to record neural activity in mice while presenting a variety of visual stimuli, facilitating a comprehensive understanding of visual processing mechanisms. </p>
                        <br>
                        <p>
                            By creating a one-to-one mapping between the text tokens generated by image-to-text models and the most active neurons, we can quantify the neural encoding capabilities of the model.
                        </p>
                    </div>
                    <div class="button-container">

                        <button id = 'mouse_btn' onclick="showSection('mouse', this)" class="navigate-button default-focused">Mouse Visual Cortex</button>
                        <button id = 'primate_btn' onclick="showSection('primate', this)" class="navigate-button">Primate Visual Cortex</button>
                        <!-- <button id = 'resting_btn' onclick="showSection('resting', this)" class="navigate-button">Resting Analysis</button> -->
                    </div>
                    <div id="mouse" class="behavior-section post-container">
                        <img src="figures/Figure_2_NeuroGPT_CVPR.drawio.svg" alt="mouse Image" class="inline-image">
                        <p class="caption">Figure 2: Qualitative outputs of the neural encoding benchmark for assessing image2text models neural encoding performance on mouse visual cortex. </p>
                    
                            
                        <div class="post-container">
                            <div class="post-text-2">
                            <p>
                                Each image2text model possesses a unique visual feature encoding capability that reflects its sensitivity to detecting and encoding subtle visual changes through distinct captions. To quantify the visual encoding abilities of state-of-the-art image2text models, we extract UMAP embeddings for all text tokens generated by the model using the Natural Movie Dataset, as illustrated in Figure 2(A). A dense representation with tightly clustered points indicates higher sensitivity, whereas larger, more dispersed clusters suggest weaker model performance.
                            </p>
                            <br>
                            <p>
                                In order to validate the dynamic correlation between the image2text models and neural visual feature encoding, we analyze the top active overlapping neurons for preferred stimuli (Figure 2(B)) and observe that the visual cortex clusters similar stimuli, with specific neurons showing sensitivity to subtle visual changes. 
                            </p>
                            <br>
                            <p>
                                Although a dynamic correlation exists between the image2text models and the neural visual feature encoding mechanisms, we utilize our benchmark to evaluate the neural encoding capabilities of an image2text model. As shown in Figure 2(C), we assess model performance based on the percentage of wins, revealing that models with dense and expansive latent spaces exhibit superior neural encoding capabilities. Additionally, these models generate a higher number of unique captions for the Natural Movie Dataset, as illustrated in Figure 2(D). 
                            </p>
                            <br>
                            <p>
                                Our mappings also reflect the neural profiles in the visual cortex. We observe diverse spiking profiles among neurons (Figure 2(E)), with each encoding a varying number of tokens, indicating sensitivity to a range of contrasting images. 

                                As a qualitative performance measure, in Figure 2(F) we show the attention maps from our best-performing image2text model for various images, revealing that these maps resemble Gaussian fields rather than point clouds, closely aligning with the receptive fields of visual cortex neurons. 

                            </p>
                        </div>
                        </div>

                    </div>
                    <div id="primate" class="behavior-section post-container" style="display:none;">
                        <img src="figures/Figure_3_NeuroGPT_CVPR.drawio.svg" alt="primate Image" class="inline-image">
                        <p class="caption">Figure 2: Qualitative outputs of the neural encoding benchmark for assessing image2text models neural encoding performance on primate visual cortex. </p>
                        
                        <div class="post-container">
                            <div class="post-text-2">
                            <p>
                                Utilizing primate neural responses, we further validate our benchmarking technique across different species. Similar to the trends observed in mouse neural responses, our findings indicate that models exhibiting a higher sensitivity to detecting visual changes also demonstrate improved neural encoding capabilities, as illustrated in Figure 3. This consistency across species underscores the robustness of our approach and its applicability in understanding visual processing in both primates and mice.
                            </p>
                            </div>
                            
                        </div>
                    </div>

                    </div>
                    
                </div>
            </article>
            <article>
                <div class="post-container">
                    <div class = "post-text-2">
                        <h2>Conclusion</h2>
                        <p>
                            Our approach offers a novel framework for benchmarking state-of-the-art image captioning models based on their ability to encode neural activity from the visual cortex. By systematically mapping neural responses to text tokens generated by image2text models, we present a unique evaluation criterion that aligns with the visual cortex’s functionality. 
                            This has significant implications for applications such as natural language-guided visual therapy, providing both qualitative and quantitative metrics to assess image2text models using mammalian visual cortex data. 
                        </p>
                        <br>

                        <h6>Learn more</h6>
                        <div class="info-button-container">
                            <ul>
                                <button class="info-button" onclick="navigateTopaper()">
                                    Read our paper in NeurIPS 2024<i class="fas fa-external-link-alt"></i>
                                </button>
                            </ul>
                        </div>

                        <div class="noto center-col demo post-container">
                            <form action="https://formspree.io/f/xvoezewb" method="POST" id="#demo-form" style="width: 100%">
                                <div class="center-col" style="width: 73%; row-gap: 10px;">
                                    <input class="input-style-1" type="text" placeholder="you@your-work-email.com" name="email" style="width: 100%;" required>
                                    <!-- <textarea class="input-style-1" type="text" placeholder="Your Message (Optional)" name="message" style="width: 100%; height: 70px;"></textarea> -->
                                    <button type="submit" class="button" style="width: 100%;">Request a demo</button>
                                </div>
                            </form>
                        </div>
                        

                    </div>
                </div>
                
            </article>

            
            <!-- Add more articles as needed -->
        </section>
    </main>
    <footer>
        <div class="footer-container center-col" style="margin-top: 30px;">
            <!-- <div class="social-follow">
                <span>Follow us on</span>
                <a href="https://x.com/tibbtech?mx=2" target="_blank"><i class="fa-brands fa-x-twitter"></i></a>
            </div> -->
            © Tibbling Technologies · All Rights Reserved
        </div>
    </footer>
    

    <script>
        var nav_open = false
        let hidden_nav = document.getElementById("hidden-nav")
        // document.getElementById('mouse_btn').addEventListener("focus", focusFunction);
        // document.getElementById("mouse_btn").focus()
        
        document.addEventListener('click', (e) => {
            if (nav_open && !hidden_nav.contains(e.target)) {
                toggleNav()
            }
        })

        function toggleNav(e) {
            nav_open = hidden_nav.style.right == "0px"
            hidden_nav.style.right = nav_open ? "-300px" : "0px"
            nav_open = !nav_open

            // we do not want this event to propagate to the window event listener
            e?.stopPropagation()
        }

        function scrollEle(ele) {
            toggleNav()
            document.getElementById(ele).scrollIntoView({ behavior: 'smooth', block: "center", inline: "nearest" })
        }

        // Slider functionality
        // var slideIndex = 0;
        // showSlides(slideIndex);

        // function changeSlide(n) {
        //     showSlides(slideIndex += n);
        // }
        
        // function showSlides(n) {
        //     var i;
        //     var slides = document.getElementsByClassName("main-image");
        //     if (n >= slides.length) { slideIndex = 0 }
        //     if (n < 0) { slideIndex = slides.length - 1 }
        //     for (i = 0; i < slides.length; i++) {
        //         slides[i].style.display = "none";
        //     }
        //     slides[slideIndex].style.display = "block";
        // }

        // Automatic slideshow
        // setInterval(function() {
        //     changeSlide(1);
        // }, 3000); // Change image every 3 seconds

        
        // function focusFunction(){
        //     //Write your script here
        //     console.log('Focused');
        //     document.getElementById("mouse_btn").style.backgroundColor = "red";
        // }
        // Function to show the selected behavior section
        function showSection(section, clickedButton) {
            var sections = document.getElementsByClassName("behavior-section");
            for (var i = 0; i < sections.length; i++) {
                sections[i].style.display = "none";
                }
            
            document.getElementById(section).style.display = "block";
            
            var buttons = document.getElementsByClassName('navigate-button');
            console.log(buttons);
            for (var i = 0; i < buttons.length; i++) {
                buttons[i].classList.remove('default-focused');
            }
            // Add active class to the clicked button
            clickedButton.classList.add('default-focused');
        }

        
        document.addEventListener('DOMContentLoaded', function () {
            var video = document.getElementById('AnimalFormer_video');
            video.playbackRate = 2;
            video.play();
        });

        function navigateTopaper() {
            window.open('https://openreview.net/pdf?id=sGLqjAVep4',  '_blank'); // Replace with your desired URL
        }

        function sendEmail() {
            var email = 'asim@tibbtech.com'; // Replace with your email address
            window.location.href = `mailto:${email}`;
        }
  
        
    </script>
</body>
</html>
